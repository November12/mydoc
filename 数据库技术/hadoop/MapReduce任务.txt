1. job和task  一个job(作业)会拆分为若干个task(任务)，主要包括map task和reduce task, 还有初始化和最后清理的task。  jobtracker就是分配这些task到tasktracker slots上执行。  每个tasktracker，都有一些slots(分为map slots/reduece slots)，也就是资源的抽象。2. Job类   Job class组合了job所需的所有要素。可以设置很多的class，参与job。  1) map阶段  setMapperClass  setCombinerClass  setMapOutputKeyClass  setMapOutputValueClass  2) reduce阶段  setNumReduceTasks                       // 设置reduce任务数  setPartitionerClass                     // 确定key发往哪个reduce执行, partition和reduce的数量要一致  setSortComparatorClass                  // 设置排序方法  setGroupingComparatorClass              // 把同一组key的value放在一起，key是同一组key中的第一个key  setReducerClass  setOutputKeyClass  setOutputValueClass  3) 输入输出阶段  setInputFormatClass  setOutputFormatClass  4) 优化  setSpeculativeExecution                 // 推测执行  setMapSpeculativeExecution  setReduceSpeculativeExecution3. Job的输入  3.1 InputSplit    在input目录下，可能是一堆的输入文件。首先需要对这些文件进行split。    方法一  分割文件 (类似切大饼)      split的大小为 [mapred.min.split.size, block size]      当然，哪怕文件的size很小，也是一个split。      显然，每个输入文件的大小如果不一样，切出来也大小也不一样。    方法二  压缩文件      如果后缀是.gz和.lzo，则直接分给mapper来处理，不切    方法三  小文件      如果都是小文件，按照方法一，显然会造成很多的split。增加的map的数量，这样很不合理。      所以对于小文件，可以重载InputFormat.isSplitable(), 再通过RecordReader读取多个文件，再切    要说明的是，分割完全是靠算法执行的，和具体内容无关，也就是说，可能分割点正好在<key,value>中间。    这个过程不需要读文件内容，所以速度很快。    3.2 RecordReader    InputFormat会调用RecordReader, 它负责处理记录的边界情况以及把数据表示成keys/values对形式。    简单的说, InputFormat把input file送给RecordReader加工，出来的就是一个个的keys/values了    所以，重载RecordReader可以解释input file    由于分割的时候，可能恰好会分割到<K,V>中间。因此，在初始化的时候，会将第一行skip掉。   4. Job的输出  4.1 输出控制    默认的, reduce会把输出写入到一个文件当中，如part-00000, part-00001，这样并没有对输出进行分类。    比如，想按照每个气象站，每年一个文件，这要怎么做呢。    很简单，重载MultipleOutputFormat.generateFileNameForKeyValue()来控制。    延生一下，我们可以控制输出的目录名和文件名，还可以把一个<K,V>输出到多个文件中。  4.2 数据库输出    可以通过TableOutputFormat，把结果直接写入到HBase。(输入同理)    或者通过Sqoop，直接写入到关系型数据库。(输入同理)    5. Job的工作机制  一个作业会有以下几个步骤:    1) client -> JobTracker      向JobTracker获取新的job ID      检查输出的合法性 (连目的地都搞不清楚，就没必要执行了)      计算split (原以为是JobTracker做的，原来是client, jobtracker还真懒)      将job的JAR文件、配置文件、spilt后的task信息复制到jobtracker/jobID/下         (注意，同一个task，可能会有多个副本，以供jobtracker挑选，副本数由mapred.submit.replication定义)      可以看到，整个任务的分配，全都是在客户端进行的，jobtracker只负责调度。    2) JobTracker      根据提交的信息, jobtracker可以创建相应的map任务和reduce任务 (数量都是由client决定的)    3) JobTracker -> TaskTracker      tasktracker会主动发送心跳包，一是告诉jobtracker我还活着，再一个就是告诉jobtracker自己的任务执行情况      因此，如果jobtracker发现tasktracker有空闲slot的时候，就会给他分配任务      分配任务有其算法，简单的说:        map任务优先于reduce任务        数据 local > rack-local(机架) > remote      每类型的任务比例，会通过内置计数器记录，以供优化。    4) tasktracker 执行      让我们先思考一个问题，要执行一个task，需要哪些东西?         JAR包        所需要处理的数据      tasktracker会在本地创建一个目录，将JAR包解压缩后放进去      为了确保软件问题不会互相干扰，tasktracker会启动一个新的JVM来执行任务。        (启动JVM可能需要1秒钟，虽然可以重用，但我觉得，这个开销是十分值得的)      新建的子进程会和父进程通讯(几秒钟一次)，告知运行状态，tasktracker也会统计汇报给jobtracker。(级级汇报)      值得一提的是，还可以通过标准输入输出(Streaming)和套接字(Pipes), 让其它的程序(如C++)来执行map和reduce任务。    6. 失败处理  6.1 task失败    我们知道，task是在子进程中执行的。如果task退出，或者JVM崩溃，tasktracker可以知道，并做相应的处理。    但如果task被卡住了，比如进入死循环，或阻塞到某个地方，这种情况就只能靠timeout。      task子进程会定时上报进度信息，无论是map还是reduce，只要读写<K,V>或者设置Reporter，都会上报进度。      这个进度，也会当做心跳。若任务超过mapred.task.timeout/10分钟, 没有上报信息，则会认为任务失败。(可见，向领导汇报是十分重要的)    如果一个task失败，jobtracker会重新分配mapred.map.max.attempts/4次，mapred.reduce.max.attempts/4次    如若还是失败，则整个job失败。    当然，有些场景下，我们未必希望一个task失败，整个job就算作失败。这时可以通过调节失败百分比来控制。      mapred.max.map.failures.percent, mapred.max.reduce.failures.percent  6.2 task中止(killed)    killed和失败不同，killed是tasktracker主动终止的。    原因可能是推测副本(你没价值了)，或者通过Web UI终止。    所以这种情况，不能简单的归纳为失败  6.3 tasktracker失败    如果tasktracker在mapred.tasktracker.expiry.interval毫秒内，没有上报心跳给jobtracker。(不汇报肯定是不行的)    或者这个tasktracker上的失败率非常高。(业绩不好)    那么jobtracker就会认为此tasktracker失败，并将此tasktracker移除。    tasktracker可以通过重启重新上岗。  6.4 jobtracker失败    jobtracker失败了怎么办? 不要问我，因为我也不知道。    jobtracker是个单点问题，它失败了，肯定所有的任务都失败。    目前可以说是无解的，所以要给它安排一台好机器。      7. Job的调度  老板希望早上8点前，看到前一天的统计报告。而前一天的数据，只有到了凌晨才能汇集上来。留给小飞的时间只有8个小时。  小飞埋头3天3夜，经过不断优化，终于将Job的运行时间，控制在了4个小时以内。  小飞信心勃勃的对老板承若，8点前一定统计好数据。  可是，到了早上8点，发现还没有跑完，小飞傻了眼，原来slots被其他程序占用了。    可以通过设置 mapred.jobtracker.taskScheduler，来选择不同的调度器。    7.1 FIFO(默认)    FIFO，另外还支持优先级。(个人认为，优先级是一种很龊的方式，你会认为你任务的优先级比别人低么?)    当然，优先级只在分配任务的时候才起作用。如果有一个低优先级的任务在那里缓慢的跑着，后面的任务优先级再高也没用。    7.2 Fair Scheduler(公平调度器)    每个用户(并非job)使用slots的机会是相同的，没有优先级的概念。    某个用户的job多，job运算时间长，都不会获得更多的资源。    因此，短的job会更快的结束，长的job会一直慢慢的运行。      7.3 Capacity Scheduler    极左(FIFO)极右(Fair)之后，总会有一个中庸的方案。    那就是队列(queue)的概念。某个用户或组织提交的任务放到某个预先设定的队列上。    每个队列先分配一些slots，队列内部使用FIFO。    这样做，其实就是为某些用户，分配某些资源。    小飞恍然大悟，急忙跑到老板办公室，对老板说，早上8点前可以完成，但必须分配一个queue给我。(所以说，拍胸脯的时候，也就是最容易要资源的时候)    8. 神奇的聚合  我们知道，map的输出，就是reduce的输入。  但在这两个过程之间，其实框架帮我们做了很多很多的工作。  这些工作，也是MapReduce最核心的。正是因为有了这些，神奇的事情发生了。。。    1) 当数据从map输出，并不是直接的写入磁盘。数据首先写到内存中的一个缓冲区，并做了一些预排序处理。io.sort.mb/100MB  2) 但缓冲区中的数据到达了一个阀值(io.sort.spill.percent/0.8)，就会把数据spill到磁盘，这个过程不是阻塞的，可以一边写一边spill。     当然，如果缓冲区写满了，还是会阻塞的。  3) spill的过程，也不是顺序去写的，写之前会进行一个二次快速排序。     首先根据partition排序，partition内再根据key排序。输出包括一个索引文件和数据文件。     Combiner是在这个时候介入的，排序之后，保存文件之前。(之后的归并，也会执行Combiner)     spill文件保存在由mapred.local.dir指定的目录中，当所有的Map任务结束后删除。  4) 每当达到阀值时，都会产生spill文件。所以，当一个Map执行完以后，会有多个spill文件。     通过多路归并，会归并为一个spill文件(包括一个索引文件和数据文件)。     最大归并路数由io.sort.factor/10控制     写入文件时，可以用mapred.compress.map.output和mapred.map.output.compression.codec来进行压缩。(显然，这是非常必要的)  5) 到此，map的输出文件已经准备好了。Reducers会通过HTTP来获取对应的数据。(只获取自己需要的，并非所有数据)     负责传输的工作线程数tasktracker.http.threads/40，这点就像网络蚂蚁一样。  6) 这里要说明的是，reducer需要的数据零零碎碎的分布在map上，而且这些map肯定不能在同一时间都完成。     所以收集数据阶段要持续一段时间(复制阶段)，可以设定mapred.reduce.parallel.copies/5来控制拷贝线程数。     即使拷贝成功了，map上的文件也不能删除，因为别的map文件还没准备好，有可能需要重新拷贝。只有整个job都执行完成，才能删除。  7) 如果reduce收集的输入数据很小，会直接放到内存缓冲区。(mapred.job.shuffle.input.buffer.percent)     如果缓冲区到达一定阀值(mapred.job.shuffle.merge.threshold)，或者到达map输出的阀值(mapred.inmem.merge.threshold)，缓冲区的数据会spill到磁盘。  8) 和map阶段一样，多个spill文件也需要排序归并(排序阶段)，这个归并不是在所有map文件都接收后才执行，而是边传边执行。     而且最终的结果，也并非要合并为1个文件，可以是多个而不再合并(io.sort.factor/10, 即55个文件，最后合并6次)。     显然这些都是处于效率考虑。  9) 至此，reduce的输入已经准备就绪。reduce的输出将会保存到本地的datanode上。        9. Job的执行  9.1 speculative execution (推测执行)    对于已经运行了一段时间(>1分钟)的task，可能会再另一个tasktracker上执行。当然，前提是没有其它尚未执行的task的情况。    实践证明，这个设置并未起到很好的效果，如果job是独享集群的，就开着吧，没坏处(默认是开的)。    如果资源比较紧张，建议还是关闭这个设置。      9.2 JVM重用    JVM重用机制，最大的好处还不在于节省JVM启动的那1秒钟时间，更多的是可以设置静态字段，让后续的操作可以使用到，从而节省时间。    如果不是因为这个，而仅为了纠结那1秒钟，建议还是不要重用了，能规避很多奇怪的错误。      9.3 异常数据    当输入文件中，有异常的数据怎么办?    抛出exception么，这样绝对不行，因为excepion以后，程序就异常退出了，从而job停止。    所以，这些问题，都必须通过map&reduce程序去处理掉。    10. 临时文件命名  hadoop必须保证，每次执行的中间文件，其文件名不能冲突。  所有的命名，由jobtracker统一管理，并做了如下规则:    job_201403171643_0006    task_201403171643_0006_m_000000    attempt_201403171643_0006_m_000000_0  这样，每一个job里面，可以有很多个task，每个task，也可以重复尝试多次。  11. 计数器  通过一些计数器，是获取一些运行状态信息的最简单的方法。对于质量控制、应用统计、诊断故障等方面都有着重要的作用。  需要说明的是，task子进程汇报进度的时候，每次的汇报都是全量而非增量，由jobtracker统计，这样做避免了某次上报失败带来的影响。  当某个task运行失败时，此task已计数部分会消去，导致web UI显示值突然变小。    11.1 内置计数器    内置计数器非常多，在了解原理以后，也很容易理解，就不复述了。平时多看看，很宝贵的信息。    可分别以job, map, reduce为基本单位，进行查看。      11.2 用户定义的计数器    可通过map()/reduce()中的reporter对象，添加自定义计数器    但统计的结果只能通过系统日志的方式打印。而且只有在job结束的时候才能看到。    如果觉得不方便，还可以做一个监控程序，通过getcounter()函数输出。上面的两个问题(打印方式，动态获取)就都解决了。12. 排序  排序是MapReduce的核心技术，尽管大多数应用本身并不需要排序，但为了组织数据，排序不可避免。  换句话来说，你是不需要排序，但你需要reduce吧，需要聚合吧。不排序，又怎么能做到聚合呢。  以下这些过程的排序，我们都可以自定义来控制。    12.1 部分排序    为了确保map的输出，最终能流入到指定的reduce。所以，当spill的时候，要先partition。    对于每一个partition内部，进行排序。最终得到的文件，是一个分块的，部分排序的文件。      12.2 全排序    全排序原理很简单，主要是在partition的时候做点手脚，分为[0 100], [101, 200]这样的连续区间。再进行部分排序。    这里最大的问题不是排序，而是区间的划分。如果按照数学意义上的等分来划分的话，势必造成partition的不均匀。(partition代表着reduce的工作量)    所以，这里需要引入一个采样器。通过统计采样，大致算出数据分布，并按照此分布进行分区。      12.3 辅助排序(二次排序)    上面讨论的都是对key的排序，但某些场景，还需要对值排序。    比如，统计某用户的功能使用流水，并对其关联性进行分析统计。    这里，用户是key，value中期望按照时间戳排序。(不能使用user,datetime作为key，这样可能被送到不同的reduce当中)      13. Join操作  join可是关系型数据库的强项，到了hadoop，数据分布在不同的机器，想做join就不容易了。  join的操作，可以在map端，也可以在reduce端操作。    13.1 map side join    如果有小表，小到可以将全量数据放入内存的话，那么只将大表做input，在map时join内存中的小表即可。reduce可以省去。    另一个思路是，利用bloom过滤器，将大表中无需join的数据剔除，再利用reduce进行join。    可见, map size join是需要前提的。      13.2 reduce side join    reduce端的join，原理很简单，把所有需要join的数据，都作为input。    确保map函数输出的key一致。这样，到了reduce，同样的key就可以形成一个valuelist，将其join即可。    这样做join，对输入并没有限制，因此非常通用。    问题是，所有的数据，都需要原封不动的输出到reduce，网络开销很大。    14. 边数据(side data)  side data指的是作业所需的额外的只读数据。目标是，让所有的map和reduce都能高效的使用到这些数据。  14.1 Configuration.getProperty()    如果只有少量的<K,V>数据，完全可以通过getProperty()，放到Configuration里面。    使用时，通过Context中的getConfiguration()来获取。      14.2 分布式缓存    再来看看更普遍的做法。利用hadoop的分布式缓存设计。    其原理是，运行job时，带上-files或-archieves选项，可以将文件(本地、HDFS)复制到jobtracker上。    当tasktracker执行任务时，会将需要的文件，从jobtracker上复制一份到本地，并放入内存。    根据使用计数器更新这些缓存，在绝大多数情况下，tasktracker只需要加载一次即可。    另外一种操作方式是，使用distributed cache API来操作。      14.3 利用JVM重用机制    通过设置静态字段，来重复利用。                                                                                                                                                