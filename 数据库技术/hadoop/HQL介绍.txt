最最完整的语法规则，可以查阅这里。http://wiki.apache.org/hadoop/Hive/LanguageManual

1. 语法规则
  hive也有database的概念，是一个逻辑概念，仅解决命名冲突问题。
  CREATE DATABASE financials;
  SHOW DATABASES;
  USE DATABASE;
  USE default;

  hive对大小写不敏感

  一次命令(可批量)以分号作为结束
  不区分大小写
  
  非交互模式:
    hive -f script.q
    hive -e 'select * from dummy'
    
4. 数据类型
  基本数据类型:
    tinyint     1字节有符号
    smallint    2字节有符号
    int         4字节有符号
    bigint      8字节有符号
    float       4字节
    double      8字节
    boolean     true/false
    string      字符串
  复杂数据类型:
    array       有序字段，类型必须相同
    map         无序key/value, key和value的类型必须相同
    struct      一组命名字段
    
    
2. 元数据查看
  show tables;
  show functions;             # 显示内置函数 
  decs 
    

2. DDL 操作
  1) 创建表
    create table optional_user_day (user string, datetime bigint)
    row format delimited fields terminated by ','
    lines terminated by '\n' stored as textfile;
    
3. DQL 操作
  [WITH CommonTableExpression (, CommonTableExpression)*]
  SELECT [ALL | DISTINCT] select_expr, select_expr, ...
  FROM table_reference
  [WHERE where_condition]
  [GROUP BY col_list [HAVING condition]]
  [   CLUSTER BY col_list
    | [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list]
  ]
  [LIMIT number]
  
  WHERE：    详见 https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF
  DISTINCT： 不是加载到某一列，而是加载到所有列。
  FROM：     可以是一个表，也可以是一个子查询。
  COLUMN     表达方式可以使用正则, 如 SELECT `(ds|hr)?+.+` FROM sales
  SORT BY：  只保证单个reduce输出有序
  ORDER BY： 全排，只有一个reduce任务
  DISTRIBUTE BY:    指定的数据内容分到同一个reducer
  CLUSTER BY:       Cluster By is a short-cut for both Distribute By and Sort By.
  JOIN:      join的条件只能是等号。(机制决定，很难改变)
             join只支持 [left, right, full] outer join 和 left semi join
  UNION ALL: 支持多表Union操作
  TABLESAMPLE:     支持取样语法，前提是已经分桶
  VIRTUAL COLUMNS: 目前只有两个, INPUT__FILE__NAME 和 BLOCK__OFFSET__INSIDE__FILE
  LATERAL VIEW：   将Array类型的列展开后统计
  CTE：


3. DML 操作
  hive不支持INSERT INTO, UPDATE, DELETE操作
  2) 加载数据:
    # 加上overwrite，会将数据库目录清空。不加则为添加
    LOAD DATA LOCAL INPATH 'input/ncdc/micro-tab/sample.txt' overwrite into table records;
  3) SELECT
    select year, MAX(temperature) from records where temperature = 30 group by year;
    select的结果，可以放在多个位置:
    1. 放到另一个table中
      INSERT OVERWRITE TABLE test SELECT uid,name from test2;
    2. 放到本地文件系统
      INSERT OVERWRITE LOCAL DIRECTORY '/tmp/reg_3' SELECT a.* FROM events a;
    3. 放到hdfs文件系统
      INSERT OVERWRITE DIRECTORY '/tmp/hdfs_out' SELECT a.* FROM invites a WHERE a.ds='<DATE>';
  4) Load/Insert
    1. Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。
      LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]
    2. 和Select配合，将Select结果导入table
    3. Select结果也可以导入到本地目录
  
    

    
6. 



    
    







创建表：
hive> CREATE TABLE pokes (foo INT, bar STRING); 
        Creates a table called pokes with two columns, the first being an integer and the other a string

创建一个新表，结构与其他一样
hive> create table new_table like records;

创建分区表：
hive> create table logs(ts bigint,line string) partitioned by (dt String,country String);

加载分区表数据：
hive> load data local inpath '/home/hadoop/input/hive/partitions/file1' into table logs partition (dt='2001-01-01',country='GB');

展示表中有多少分区：
hive> show partitions logs;

展示所有表：
hive> SHOW TABLES;
        lists all the tables
hive> SHOW TABLES '.*s';

lists all the table that end with 's'. The pattern matching follows Java regular
expressions. Check out this link for documentation http://java.sun.com/javase/6/docs/api/java/util/regex/Pattern.html

显示表的结构信息
hive> DESCRIBE invites;
        shows the list of columns

更新表的名称：
hive> ALTER TABLE source RENAME TO target;

添加新一列
hive> ALTER TABLE invites ADD COLUMNS (new_col2 INT COMMENT 'a comment');
 
删除表：
hive> DROP TABLE records;
删除表中数据，但要保持表的结构定义
hive> dfs -rmr /user/hive/warehouse/records;

从本地文件加载数据：
hive> LOAD DATA LOCAL INPATH '/home/hadoop/input/ncdc/micro-tab/sample.txt' OVERWRITE INTO TABLE records;

显示所有函数：
hive> show functions;

查看函数用法：
hive> describe function substr;

查看数组、map、结构
hive> select col1[0],col2['b'],col3.c from complex;


内连接：
hive> SELECT sales.*, things.* FROM sales JOIN things ON (sales.id = things.id);

查看hive为某个查询使用多少个MapReduce作业
hive> Explain SELECT sales.*, things.* FROM sales JOIN things ON (sales.id = things.id);

外连接：
hive> SELECT sales.*, things.* FROM sales LEFT OUTER JOIN things ON (sales.id = things.id);
hive> SELECT sales.*, things.* FROM sales RIGHT OUTER JOIN things ON (sales.id = things.id);
hive> SELECT sales.*, things.* FROM sales FULL OUTER JOIN things ON (sales.id = things.id);

in查询：Hive不支持，但可以使用LEFT SEMI JOIN
hive> SELECT * FROM things LEFT SEMI JOIN sales ON (sales.id = things.id);


Map连接：Hive可以把较小的表放入每个Mapper的内存来执行连接操作
hive> SELECT /*+ MAPJOIN(things) */ sales.*, things.* FROM sales JOIN things ON (sales.id = things.id);

INSERT OVERWRITE TABLE ..SELECT：新表预先存在
hive> FROM records2
    > INSERT OVERWRITE TABLE stations_by_year SELECT year, COUNT(DISTINCT station) GROUP BY year 
    > INSERT OVERWRITE TABLE records_by_year SELECT year, COUNT(1) GROUP BY year
    > INSERT OVERWRITE TABLE good_records_by_year SELECT year, COUNT(1) WHERE temperature != 9999 AND (quality = 0 OR quality = 1 OR quality = 4 OR quality = 5 OR quality = 9) GROUP BY year;  

CREATE TABLE ... AS SELECT：新表表预先不存在
hive>CREATE TABLE target AS SELECT col1,col2 FROM source;

创建视图：
hive> CREATE VIEW valid_records AS SELECT * FROM records2 WHERE temperature !=9999;

查看视图详细信息：
hive> DESCRIBE EXTENDED valid_records;



