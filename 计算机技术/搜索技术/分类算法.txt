0. 在机器学习领域，分类的目标是指将具有相似特征的对象聚集。

考虑因素:
  先验概率
    也就是事先的样本及结果的准备。很多情况下，这些样本可能是缺失的。(赶工准备容易造成误判，反而给机器学习以误导)
  样本容量
    样本的多与少，能给出很多样本，还是只能给出少量样本。
  输入参数
    信源究竟可以提供哪些参数。实际生产过程中，能给出的参数，往往很少。
  可信度
    你的老板和用户，往往只看结果，对可信度并不关心。
    但某些场景下，可能需要提及原理，以及估算。




1. 决策树(Decision Trees)
  介绍:
    决策树是一个树结构(可以不是二叉树)，其每个非叶节点表示一个特征属性上的测试，每个分支代表一个值域，每个叶节点存放一个类别。
  优点:
    非常容易理解和解释。
    易于通过静态测试，测量其模型的可行度。
    无需样本训练，短时间内即可做出效果良好的结果。
  缺点:
    属性选择度量具有一定难度(甚至于来说很理想化)
    无学习能力，对未知结果无法进行有效二次判断。
    假设属性间是独立的。

2 人工神经网络(artificial neural network, ANN)
  介绍:
    一种非常复杂的算法。
    人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统。
    单个神经元的作用是把一个n维向量空间用一个超平面分割成两部分(称之为判断边界)，给定一个输入向量，
    神经元可以判断出这个向量位于超平面的哪一边。
  优点：
    分类的准确度高
    并行分布处理能力强
    分布存储及学习能力强
    对噪声神经有较强的鲁棒性和容错能力
    能充分逼近复杂的非线性关系
    具备联想记忆的功能
  缺点：
    神经网络需要大量的参数，如网络拓扑结构、权值和阈值的初始值。
    不能观察之间的学习过程，输出结果难以解释，会影响到结果的可信度和可接受程度。
    学习时间过长,甚至可能达不到学习的目的。


4 KNN算法(K-Nearest Neighbour)
  介绍:
    全名叫K最邻近结点算法。很好理解，寻找距离最近的K篇文本。通过K篇文本的类别，得出新文本的类别。
    K的取值非常考究，不同的K值最终可能得到不同的结果。
    K值的确定没有什么科学方法，一般初始取值几百~几千，之后根据结果再进行调整。
  优点:
    对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。
    重新训练的代价较低
  缺点:
    当一个样本容量很大时，导致新样本附近都是此样本。不过可以采用权值来改进。
    计算量较大。(每次都要和样本全集进行比较)
    要求样本容量较大。
    

5 支持向量机(Support Vector Machine, SVM)
  介绍:
    使用简单的线形分类器划分样本空间。对于在当前特征空间中线形不可分的模式，则使用一个核函数把样本映射到一个高维空间中，使得样本能够线形可分。
    它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器。
  优点：
    对样本数量要求不高
    可以提高泛化性能。
    可以解决高维问题。
    可以解决非线性问题。
    可以避免神经网络结构选择和局部极小点问题。
  缺点：
    对缺失数据敏感。
    对非线性问题没有通用解决方案，必须谨慎选择Kernelfunction来处理。


6 朴素贝叶斯分类器(Naive Bayesian Model，NBC)
  介绍: 
    贝叶斯分类的基础是概率推理。每个数据样本，为n维特征向量，即X={x1,x2,..,xn}。
    贝叶斯分类，假定n维向量彼此独立。(事实上往往这点不成立，因而影响了准确度)
    举个例子，如果一种水果其具有红，圆，直径大概4英寸等特征，该水果可以被判定为是苹果。
    需要知道先验概率，这个可以通过大量的机器学习来完成。
  优点: 
    结果相当稳定。
    算法简单。
    估算的参数要求少，容易实现。
  缺点:
    假设属性独立的要求。
    存在一定的错误率，虽然很低。


7 Adaboosting算法
  资料:
    http://www.inf.fu-berlin.de/inst/ag-ki/adaboost4.pdf
  介绍:
    Adaboosting算法是将很多个分类器(弱分类器)的意见有效的结合起来, 形成一个强分类器。
    理论证明，只要每个弱分类器分类能力比随机猜测要好，当其个数趋向于无穷个数时，强分类器的错误率将趋向于零。
    算法要做两件事情:
      1. 从众多候选分类器中筛选出分类器。
      2. 计算赋予出的分类器权值。
  优点:  
    adaboost是一种有很高精度的分类器。
    可以使用各种方法构建子分类器，Adaboost算法提供的是框架。
    当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单。
    简单，不用做特征筛选。
    不用担心overfitting。



